{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1387b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding1 torch.Size([1, 4, 8])\n",
      "embedding2 torch.Size([1, 4, 8])\n",
      "token_embedding torch.Size([1, 5, 8])\n",
      "token_embedding torch.Size([1, 5, 8])\n",
      "output torch.Size([1, 5, 8])\n",
      "cls_token_output torch.Size([1, 8])\n",
      "loss tensor(2.9624, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def img2embed(image, patch_size, stride, weight):\n",
    "    patches = F.unfold(image, patch_size, stride=stride)\n",
    "    patches = patches.transpose(-1, -2)\n",
    "    embedding = patches @ weight\n",
    "    return embedding\n",
    "def img2embed_conv(image, kernel, stride):\n",
    "    conv_output = F.conv2d(image, kernel, stride=stride)\n",
    "    b, c, h, w = conv_output.shape\n",
    "    embedding = conv_output.reshape(b, c , h * w).transpose(-1, -2)\n",
    "    return embedding\n",
    "\n",
    "bc, i_ch, i_h, i_w = 1, 3, 8, 8\n",
    "patch_size = 4\n",
    "stride = 4\n",
    "output_dim = 8\n",
    "max_num_token = 16\n",
    "image = torch.randn(bc, i_ch, i_h, i_w)\n",
    "weight = torch.randn(patch_size * patch_size * i_ch, output_dim)\n",
    "num_classes = 10\n",
    "label = torch.randint(0, num_classes, (bc, ))\n",
    "\n",
    "embedding1 = img2embed(image, patch_size, stride, weight)\n",
    "print(\"embedding1\", embedding1.shape)\n",
    "\n",
    "kernel = weight.transpose(-1, -2).reshape(output_dim, i_ch, patch_size, patch_size)\n",
    "embedding2 = img2embed_conv(image, kernel, stride)\n",
    "print(\"embedding2\", embedding2.shape)\n",
    "\n",
    "\n",
    "# step 2: CLS token embedding\n",
    "cls_token_embedding = torch.randn(bc, 1, output_dim, requires_grad=True)\n",
    "token_embedding = torch.cat([cls_token_embedding, embedding2], dim = 1)\n",
    "print(\"token_embedding\", token_embedding.shape)\n",
    "\n",
    "# step3: Positional embedding\n",
    "positional_embedding_table = torch.randn(max_num_token, output_dim, requires_grad=True)\n",
    "seq_len = token_embedding.shape[1]\n",
    "positional_embedding = positional_embedding_table[:seq_len].repeat([token_embedding.shape[0], 1, 1])\n",
    "token_embedding += positional_embedding\n",
    "print(\"token_embedding\", token_embedding.shape)\n",
    "\n",
    "# step4 feed embedding to Transformer Encoder\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=token_embedding.shape[2], nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "output = transformer_encoder(token_embedding)\n",
    "print(\"output\", out.shape)\n",
    "\n",
    "# step5 extract CLS token and classify\n",
    "cls_token_output = output[:, 0, :]\n",
    "print(\"cls_token_output\", cls_token_output.shape)\n",
    "linear_layer = nn.Linear(cls_token_output.shape[1], num_classes)\n",
    "logits = linear_layer(cls_token_output)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "loss = loss_f(logits, label)\n",
    "print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6096abbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231bac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
